"use strict";(globalThis.webpackChunkglados_documentation=globalThis.webpackChunkglados_documentation||[]).push([[265],{1187:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"Training","title":"The Machine Learning Pipeline","description":"This document explains the end-to-end process of creating the \\"Brain\\" of our AI. It covers the data lifecycle, the mathematics of learning, and the high-performance computing stack used.","source":"@site/docs/Training.md","sourceDirName":".","slug":"/Training","permalink":"/gomoku/docs/Training","draft":false,"unlisted":false,"editUrl":"https://github.com/yanisdolivet/gomoku/tree/main/documentation/docs/Training.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"Training","title":"The Machine Learning Pipeline","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Advanced Code Optimisation","permalink":"/gomoku/docs/Optimisation"},"next":{"title":"Continuous Integration & Deployment","permalink":"/gomoku/docs/Ci-cd"}}');var t=i(4848),s=i(8453);const o={id:"Training",title:"The Machine Learning Pipeline",sidebar_position:4},a="The Machine Learning Pipeline",l={},c=[{value:"1. The Learning Paradigm: Supervised Learning",id:"1-the-learning-paradigm-supervised-learning",level:2},{value:"The Concept",id:"the-concept",level:3},{value:"2. The Toolstack: PyTorch &amp; Hardware Acceleration",id:"2-the-toolstack-pytorch--hardware-acceleration",level:2},{value:"Apple Silicon Acceleration (MPS)",id:"apple-silicon-acceleration-mps",level:3},{value:"3. The Training Loop (Step-by-Step)",id:"3-the-training-loop-step-by-step",level:2},{value:"Step 1: Forward Pass",id:"step-1-forward-pass",level:3},{value:"Step 2: Loss Calculation (The Error)",id:"step-2-loss-calculation-the-error",level:3},{value:"Step 3: Backward Pass (Backpropagation)",id:"step-3-backward-pass-backpropagation",level:3},{value:"Step 4: Optimizer Step (SGD)",id:"step-4-optimizer-step-sgd",level:3},{value:"4. The Bridge: Binary Export Protocol",id:"4-the-bridge-binary-export-protocol",level:2},{value:"Why not ONNX or TorchScript?",id:"why-not-onnx-or-torchscript",level:3},{value:"Custom Binary Format (<code>.bin</code>)",id:"custom-binary-format-bin",level:3}];function h(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"the-machine-learning-pipeline",children:"The Machine Learning Pipeline"})}),"\n",(0,t.jsx)(n.p,{children:'This document explains the end-to-end process of creating the "Brain" of our AI. It covers the data lifecycle, the mathematics of learning, and the high-performance computing stack used.'}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"1-the-learning-paradigm-supervised-learning",children:"1. The Learning Paradigm: Supervised Learning"}),"\n",(0,t.jsxs)(n.p,{children:["We train our AI using a ",(0,t.jsx)(n.strong,{children:"Supervised Learning"})," approach, specifically ",(0,t.jsx)(n.strong,{children:"Imitation Learning"})," (Behavioral Cloning)."]}),"\n",(0,t.jsx)(n.h3,{id:"the-concept",children:"The Concept"}),"\n",(0,t.jsx)(n.p,{children:"Imagine observing a Grandmaster playing chess."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The Grandmaster sees the board (Input)."}),"\n",(0,t.jsx)(n.li,{children:"The Grandmaster makes a move (Target Output)."}),"\n",(0,t.jsxs)(n.li,{children:["If we observe enough games, we can learn a function ",(0,t.jsx)(n.code,{children:"f(Board) -> Move"})," that imitates the Grandmaster."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"In our case:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dataset"}),": Thousands of high-level Gomoku games (from Renju players or self-play)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Inputs"}),": The board state tensor (Is it Black's turn? Where are the stones?)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Labels (Targets)"}),":","\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Policy Target"}),": The move actually played in that situation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Value Target"}),": The final result of that game (Did Black win eventually?)."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"2-the-toolstack-pytorch--hardware-acceleration",children:"2. The Toolstack: PyTorch & Hardware Acceleration"}),"\n",(0,t.jsxs)(n.p,{children:["We use ",(0,t.jsx)(n.strong,{children:"PyTorch"}),", the leading framework for Deep Learning research.\nHowever, training a deep ResNet involves ",(0,t.jsx)(n.strong,{children:"billions"})," of floating-point operations (FLOPs). Doing this on a CPU is infeasible (it would take weeks)."]}),"\n",(0,t.jsx)(n.h3,{id:"apple-silicon-acceleration-mps",children:"Apple Silicon Acceleration (MPS)"}),"\n",(0,t.jsxs)(n.p,{children:["We exploit the ",(0,t.jsx)(n.strong,{children:"Metal Performance Shaders (MPS)"})," backend available on macOS."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"The M-Series Chips (M1/M2/M3/M4)"})," have a unified memory architecture and powerful Neural Engines / GPUs."]}),"\n",(0,t.jsxs)(n.li,{children:["By moving our Tensors to the ",(0,t.jsx)(n.code,{children:"mps"})," device (",(0,t.jsx)(n.code,{children:"tensor.to('mps')"}),"), PyTorch offloads the matrix multiplications to the GPU."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Result"}),": We achieve training speeds comparable to dedicated Nvidia GPUs, allowing us to iterate on model architecture in minutes rather than hours."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"3-the-training-loop-step-by-step",children:"3. The Training Loop (Step-by-Step)"}),"\n",(0,t.jsxs)(n.p,{children:["The training script ",(0,t.jsx)(n.code,{children:"NetworkTorch.py"})," performs the following loop thousands of times:"]}),"\n",(0,t.jsx)(n.h3,{id:"step-1-forward-pass",children:"Step 1: Forward Pass"}),"\n",(0,t.jsxs)(n.p,{children:["We feed a batch of board positions (e.g., 64 boards at once) into the network.\nThe network outputs its ",(0,t.jsx)(n.em,{children:"guess"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"Pred_Policy"}),": A probability distribution (e.g., 10% center, 0% corner)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"Pred_Value"}),": A score (e.g., +0.2)."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"step-2-loss-calculation-the-error",children:"Step 2: Loss Calculation (The Error)"}),"\n",(0,t.jsxs)(n.p,{children:["We compare the guess with the truth using ",(0,t.jsx)(n.strong,{children:"Loss Functions"}),"."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Policy Loss (Cross Entropy)"}),": Measures the distance between the predicted probability distribution and the actual move (which is a distribution with 100% on one move).","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Formula: ",(0,t.jsx)(n.code,{children:"Loss = - sum(True_Prob * log(Pred_Prob))"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Value Loss (MSE - Mean Squared Error)"}),": Measures the distance between the predicted score and the actual game result.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Formula: ",(0,t.jsx)(n.code,{children:"Loss = (True_Result - Pred_Score)^2"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.code,{children:"Total_Loss = Policy_Loss + Value_Loss"})}),"\n",(0,t.jsx)(n.h3,{id:"step-3-backward-pass-backpropagation",children:"Step 3: Backward Pass (Backpropagation)"}),"\n",(0,t.jsxs)(n.p,{children:['This is the "learning" part. Using the ',(0,t.jsx)(n.strong,{children:"Chain Rule"})," of calculus, PyTorch calculates the ",(0,t.jsx)(n.strong,{children:"Gradient"})," of the Loss with respect to every weight in the network."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"If I increase Weight #4023 by 0.001, the Error decreases by 0.0005."'}),"\n",(0,t.jsx)(n.li,{children:"It effectively computes the direction in which we should nudge the weights to make the error smaller."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"step-4-optimizer-step-sgd",children:"Step 4: Optimizer Step (SGD)"}),"\n",(0,t.jsxs)(n.p,{children:["We use ",(0,t.jsx)(n.strong,{children:"Stochastic Gradient Descent (SGD) with Momentum"}),"."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["We update all weights: ",(0,t.jsx)(n.code,{children:"Weight = Weight - Learning_Rate * Gradient"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Momentum"}),": Helps the optimization accelerate in relevant directions and dampens oscillations, like a heavy ball rolling down a hill."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"4-the-bridge-binary-export-protocol",children:"4. The Bridge: Binary Export Protocol"}),"\n",(0,t.jsx)(n.p,{children:'Once the Python model is trained, it lives in PyTorch\'s memory. We need to transfer this "intelligence" to our C++ engine.'}),"\n",(0,t.jsx)(n.h3,{id:"why-not-onnx-or-torchscript",children:"Why not ONNX or TorchScript?"}),"\n",(0,t.jsx)(n.p,{children:"Standard export formats (ONNX, TorchScript) require huge runtime libraries (libtorch is ~500MB). We want our C++ engine to be lightweight and zero-dependency."}),"\n",(0,t.jsxs)(n.h3,{id:"custom-binary-format-bin",children:["Custom Binary Format (",(0,t.jsx)(n.code,{children:".bin"}),")"]}),"\n",(0,t.jsx)(n.p,{children:"We designed a minimalistic binary protocol. The Python script acts as a serializer:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Header"}),': Writes a "Magic Number" (',(0,t.jsx)(n.code,{children:"0x5245534E"}),' - "RESN") to verify file integrity.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Metadata"}),": Writes architecture constants (Number of layers, Input channels)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Weights"}),": Iterates through every layer (Conv2d, BatchNorm) and writes the raw 32-bit floats from the GPU memory directly to the file."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["The C++ engine's ",(0,t.jsx)(n.code,{children:"loadModel"})," function is the exact mirror of this. It ",(0,t.jsx)(n.code,{children:"mmap"}),"s or reads the file linearly, reconstructing the network structure in milliseconds."]})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var r=i(6540);const t={},s=r.createContext(t);function o(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);